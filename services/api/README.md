# Procurement API Service

Backend API service for the procurement management system. Handles document processing, ETL pipelines, and data transformation.

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- PostgreSQL database
- Docker & Docker Compose

### Development Setup

1. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

2. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your credentials
   ```

3. **Start database services**
   ```bash
   npm run docker:up
   ```

4. **Run the API**
   ```bash
   python main.py
   ```

## ğŸ“ Project Structure

```
services/api/
â”œâ”€â”€ etl/                             # ETL Pipeline
â”‚   â”œâ”€â”€ transform_pipeline/          # Core transformation logic
â”‚   â”‚   â”œâ”€â”€ mappings/               # Fuzzy matching for suppliers/locations
â”‚   â”‚   â”œâ”€â”€ normalizers/            # Data cleaning and normalization
â”‚   â”‚   â””â”€â”€ transform_and_insert.py # Main ETL script
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ nanonets-ingestion/              # Document ingestion service
â”œâ”€â”€ nanonets-webhook/                # Webhook processing service
â”œâ”€â”€ edge_function/                   # Supabase Edge Functions
â””â”€â”€ requirements.txt                 # Global dependencies
```

## ğŸ”§ Available Scripts

- `python main.py` - Start the API server
- `python etl/transform_pipeline/transform_and_insert.py` - Run ETL pipeline
- `python nanonets-ingestion/main.py` - Process documents
- `python nanonets-webhook/webhook_listener.py` - Start webhook service

## ğŸš€ Deployment

- **API**: Deploy to Render
- **ETL**: Deploy as cron job
- **Webhooks**: Deploy as web service
- **Database**: Managed by Supabase 